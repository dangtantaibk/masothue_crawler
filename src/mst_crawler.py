# -*- coding: utf-8 -*-

import json
from random import randrange
import requests
from lxml import html
from loguru import logger
from lxml import etree
import pandas as pd
import re
from unidecode import unidecode
import csv
import time
import os
import math
from typing import List
import logging
from libs.user_agent import USER_AGENT
import database
import pattern

useragent = USER_AGENT[randrange(len(USER_AGENT)-1)]
# useragent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
# useragent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'
# const_headers = {'User-Agent': useragent}
const_headers = {
    'User-Agent': useragent,
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'identity',  # Kh√¥ng cho ph√©p compression
    'Cache-Control': 'no-cache',
    'Pragma': 'no-cache',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'none',
    'Upgrade-Insecure-Requests': '1'
}
const_proxies = False

ignore_text = [
    'B·ªã ·∫©n theo y√™u c·∫ßu ng∆∞·ªùi d√πng'
]

session = requests.Session()
def get_request(path_url, headers={}, proxies=False):
    url = f'{pattern.BASE_URL}{path_url}'
    # final_headers = default_headers
    logger.info(f'Send GET request:\n- URL: {url}\n- Header: {const_headers}\n- Proxy: {proxies}')
    response = requests.get(url, headers=const_headers, proxies=proxies)
    
    # Set encoding ƒë√∫ng cho ti·∫øng Vi·ªát
    response.encoding = 'iso-8859-1'

    # Parse HTML v·ªõi encoding ƒë√£ fix
    # tree = etree.HTML(response.text)

    # Debug: In ra content ƒë√£ decode ƒë√∫ng
    print(f"Correctly decoded content (first 300 chars):")
    print(response.text[:300])

    if response.status_code == 200:
        tree = html.fromstring(response.content)
        logger.info('Send GET request successfully')
        return tree, response
    else:
        logger.error(f'Failed to retrieve content. Status code: {response.status_code}')
        return False


def is_more_data(handler_page, active_page, els):
    return handler_page == active_page and els


def crawl_data_province(url, headers=const_headers, proxies=False):
    tree = get_request(url, headers, proxies)
    if not tree:
        logger.error('Failed to get data provinces')
        return


    conn = database.get_db_connection()
    cur = conn.cursor()

    # S·ª≠ d·ª•ng XPath ƒë·ªÉ t√¨m ph·∫ßn t·ª≠
    province_els = tree.xpath('//table//tr')
    data_insert = []
    for province_el in province_els:
        province_info = province_el.xpath(".//a")[0]
        province_name = province_info.text_content().strip()
        province_link = province_info.get('href')
        data_insert.append({
            "name": province_name,
            "slug": province_link
        })
        logger.info(f'- {province_name} | {province_link}')
        sql = """
            INSERT INTO mst_province (name, slug)
            VALUES (%s, %s)
        """
        cur.execute(sql, (province_name, province_link))

    database.end_db_connection(cur, conn)

    logger.info(f'Get provinces data successfully. Total: {len(province_els)}')
    return


def crawl_data_district(headers=const_headers, proxies=False):
    conn = database.get_db_connection()
    cur = conn.cursor()
    sql = """SELECT id, name, slug FROM mst_province;"""
    cur.execute(sql)
    provinces = cur.fetchall()
    for province in provinces:
        crawl_data_district_by_province(province, cur, conn, headers, proxies)
    database.end_db_connection(cur, conn)


def crawl_data_district_by_province(province_data, cur, conn, headers=const_headers, proxies=False):
    # province_data = (1, 'H√† N·ªôi', '/tra-cuu-ma-so-thue-theo-tinh/ha-noi-7') | id - name - slug
    logger.info(f'Get Province in {province_data[1]} by link {province_data[2]}')
    tree = get_request(province_data[2], const_headers, proxies)
    if not tree:
        logger.error('Failed to get data districts')
        return

    # S·ª≠ d·ª•ng XPath ƒë·ªÉ t√¨m ph·∫ßn t·ª≠
    district_els = tree.xpath('//div[@id="sidebar"]//li')
    for district_el in district_els:
        district_info = district_el.xpath(".//a")[0]
        district_name = district_info.text_content().strip()
        district_link = district_info.get('href')
        logger.info(f'- {district_name} | {district_link}')
        sql = """
            INSERT INTO mst_district (name, slug, province_id, province_name)
            VALUES (%s, %s, %s, %s)
        """
        cur.execute(sql, (district_name, district_link, province_data[0], province_data[1]))
    logger.info(f'Get districts data successfully. Total: {len(district_els)}')
    return


def crawl_data_district_by_url(url, headers=const_headers, proxies=False):
    tree = get_request(url, headers, proxies)
    if not tree:
        logger.error('Failed to get data districts')
        return

    # S·ª≠ d·ª•ng XPath ƒë·ªÉ t√¨m ph·∫ßn t·ª≠
    district_els = tree.xpath('//div[@id="sidebar"]//li')
    for district_el in district_els:
        district_info = district_el.xpath(".//a")[0]
        district_name = district_info.text_content().strip()
        district_link = district_info.get('href')
        print(f'- {district_name} | {district_link}')

    logger.info(f'Get districts data successfully. Total: {len(district_els)}')
    return


def crawl_data_career(url, headers=const_headers, proxies=False):
    conn = database.get_db_connection()
    cur = conn.cursor()


    is_more = True
    handle_page = 1
    try_count = 0
    total_crawl = 0
    while is_more:
        handle_url = f'{url}?page={handle_page}'
        try:
            tree = get_request(handle_url, headers, proxies)
            if not tree:
                logger.error('Failed to get data career')
                return
        except requests.exceptions.RequestException as err:
            logger.error(err.response.json())
            if try_count > 3:
                logger.info('Not have more data need crawl.')
                is_more = False
                return err.response.json()
            else:
                try_count += 1
                logger.warning('Try crawl more data!')

        # S·ª≠ d·ª•ng XPath ƒë·ªÉ t√¨m ph·∫ßn t·ª≠
        career_els = tree.xpath('//tbody//tr')
        for career_el in career_els:
            career_code_info = career_el.xpath(".//td[1]//a")[0]
            career_code = career_code_info.text_content().strip()
            career_name = career_el.xpath(".//td[2]//a")[0].text_content().strip()
            career_link = career_code_info.get('href')
            print(f'- {career_code} | {career_name} | {career_link}')
            sql = """
                        INSERT INTO mst_career (code, name, slug)
                        VALUES (%s, %s, %s)
                    """
            cur.execute(sql, (str(career_code), career_name, career_link))

        total_crawl += len(career_els)

        active_pages = tree.xpath('//span[@class="page-numbers current"]')
        active_page = int(active_pages[0].text_content().strip())
        if is_more_data(handle_page, active_page, career_els):
            handle_page += 1
            try_count = 0
            logger.info('Have more data need crawl')
        else:
            if try_count > 3:
                logger.info('Not have more data need crawl.')
                is_more = False
            else:
                try_count += 1
                logger.warning('Try crawl more data!')
        logger.info(f'Get districts data successfully. Total: {total_crawl}')

    database.end_db_connection(cur, conn)
    return

def crawl_data_company(crawl_by='district', headers=const_headers, proxies=False):
    logger.info(f'Get companies data by {crawl_by}')
    conn = database.get_db_connection()
    cur = conn.cursor()
    sql_by_mode = {
        'province': 'SELECT * FROM mst_province;',
        'district': 'SELECT * FROM mst_district;',
        'career': 'SELECT * FROM mst_career;',
    }
    cur.execute(sql_by_mode.get(crawl_by))
    datas = cur.fetchall()
    for data in datas:
        # district_data = (1, 'H√† N·ªôi', '/tra-cuu-ma-so-thue-theo-tinh/ha-noi-11483', 1, 'H√† N·ªôi') | id, name, slug, province_id, province_name
        crawl_data_company_by_data(crawl_by, data, cur, conn, headers, proxies)
    database.end_db_connection(cur, conn)
    logger.info(f'Get companies data by {crawl_by} successfully!')


def crawl_data_company_by_data(crawl_by, data, cur, conn, headers=const_headers, proxies=False):
    logger.info(f'crawl_data_company_by_data {crawl_by}.')
    # district_data = (1, 'H√† N·ªôi', '/tra-cuu-ma-so-thue-theo-tinh/ha-noi-11483', 1, 'H√† N·ªôi') | id, name, slug, province_id, province_name
    province_id = None
    district_id = None
    career_id = None
    url = ''
    if crawl_by == 'province':
        province_id = data[0]
        url = data[2]
    elif crawl_by == 'district':
        province_id = data[3]
        district_id = data[0]
        url = data[2]
    else:
        career_id = data[0]
        url = data[3]
    logger.info(f'crawl_data_company_by_data {crawl_by}:\n- province_id: {province_id}\n- district_id: {district_id}\n- career_id: {career_id}\n- url: {url}')

    is_more = True
    handle_page = 1
    try_count = 0
    total_crawl = 0
    while is_more:
        handle_url = f'{url}?page={handle_page}'
        try:
            tree = get_request(handle_url, headers, proxies)
            if not tree:
                logger.error('Failed to get data career')
                return
        except requests.exceptions.RequestException as err:
            logger.error(err.response.json())
            if try_count > 3:
                logger.info('Not have more data need crawl.')
                is_more = False
                return err.response.json()
            else:
                try_count += 1
                logger.warning('Try crawl more data!')

        # S·ª≠ d·ª•ng XPath ƒë·ªÉ t√¨m ph·∫ßn t·ª≠
        company_els = tree.xpath('//div[@class="tax-listing"]/div')
        for company_el in company_els:
            company_link_el = company_el.xpath("./h3/a")[0]
            company_link = company_link_el.get('href')
            company_name = company_link_el.text_content().strip()
            crawl_data_company_by_url(company_link, province_id, district_id, career_id, cur, conn, headers, proxies)

        total_crawl += len(company_els)

        active_pages = tree.xpath('//span[@class="page-numbers current"]')
        active_page = int(active_pages[0].text_content().strip())
        if is_more_data(handle_page, active_page, company_els):
            handle_page += 1
            try_count = 0
            logger.info('Have more data need crawl')
        else:
            if try_count > 3:
                logger.info('Not have more data need crawl.')
                is_more = False
            else:
                try_count += 1
                logger.warning('Try crawl more data!')
        logger.info(f'Get company data successfully. Total: {total_crawl}')
    return


def crawl_data_company_by_url(url='', province_id=None, district_id=None, career_id=None, cur=False, conn=False, headers={}, proxies=False):
    try:
        tree, response = get_request(url, headers, proxies)
        with open('debug_response.html', 'w', encoding='utf-8') as f:
            if hasattr(tree, 'tag'):
                # L·∫•y text t·ª´ response ƒë√£ decode ƒë√∫ng
                if hasattr(response, 'text'):
                    f.write(response.text)
                else:
                    html_content = etree.tostring(tree, encoding='unicode', pretty_print=True)
                    f.write(html_content)
            else:
                f.write(str(tree))
        if tree is None:
            logger.error('Failed to get data company')
            return
    except requests.exceptions.RequestException as err:
        logger.error(err.response.json())
        return err.response.json()

    # S·ª≠ d·ª•ng XPath ƒë·ªÉ t√¨m ph·∫ßn t·ª≠
    company_els = tree.xpath('//tbody//tr')
    company_name_el = tree.xpath('//thead//span') # T√™n c√¥ng ty
    company_name_globe_el = tree.xpath('//i[@class="fa fa-globe"]/../../td[2]/span') # T√™n qu·ªëc t·∫ø
    company_name_short_el = tree.xpath('//i[@class="fa fa-reorder"]/../../td[2]/span') # T√™n vi·∫øt t·∫Øt
    conpany_tax_el = tree.xpath('//i[@class="fa fa-hashtag"]/../../td[2]/span') # M√£ s·ªë thu·∫ø
    conpany_address_el = tree.xpath('//i[@class="fa fa-map-marker"]/../../td[2]/span') # ƒê·ªãa ch·ªâ
    conpany_user_el = tree.xpath('//i[@class="fa fa-user"]/../../td[2]/span/a') # Ng∆∞·ªùi ƒë·∫°i di·ªán
    conpany_phone_el = tree.xpath('//i[@class="fa fa-phone"]/../../td[2]/span') # ƒêi·ªán tho·∫°i
    conpany_active_date_el = tree.xpath('//i[@class="fa fa-calendar"]/../../td[2]/span') # Ng√†y ho·∫°t ƒë·ªông
    conpany_manage_el = tree.xpath('//i[@class="fa fa-users"]/../../td[2]/span') # Qu·∫£n l√Ω b·ªüi
    conpany_category_el = tree.xpath('//i[@class="fa fa-building"]/../../td[2]/a') # Lo·∫°i h√¨nh doanh nghi·ªáp
    conpany_status_el = tree.xpath('//i[@class="fa fa-info"]/../../td[2]/a') # T√¨nh tr·∫°ng ho·∫°t ƒë·ªông
    conpany_last_update_el = tree.xpath('//td//em') # C·∫≠p nh·∫≠t g·∫ßn nh·∫•t
    company_career_els = tree.xpath('//table[@class="table"]//tbody//tr') # Ng√†nh ngh·ªÅ kinh doanh

    company_name = len(company_name_el) and company_name_el[0].text_content().strip() or None
    company_name_globe = len(company_name_globe_el) and company_name_globe_el[0].text_content().strip() or None
    company_name_short = len(company_name_short_el) and company_name_short_el[0].text_content().strip() or None
    conpany_tax = len(conpany_tax_el) and conpany_tax_el[0].text_content().strip() or None
    conpany_address = len(conpany_address_el) and conpany_address_el[0].text_content().strip() or None
    conpany_user = len(conpany_user_el) and conpany_user_el[0].text_content().strip() or None
    conpany_phone = len(conpany_phone_el) and conpany_phone_el[0].text_content().strip() or None
    conpany_active_date = len(conpany_active_date_el) and conpany_active_date_el[0].text_content().strip() or None
    conpany_manage = len(conpany_manage_el) and conpany_manage_el[0].text_content().strip() or None
    conpany_category = len(conpany_category_el) and conpany_category_el[0].text_content().strip() or None
    conpany_status = len(conpany_status_el) and conpany_status_el[0].text_content().strip() or None
    conpany_last_update = len(conpany_last_update_el) and conpany_last_update_el[0].text_content().strip() or None

    if company_name in ignore_text:
        company_name = None
    if company_name_globe in ignore_text:
        company_name_globe = None
    if company_name_short in ignore_text:
        company_name_short = None
    if conpany_tax in ignore_text:
        company_name = None
    if conpany_address in ignore_text:
        conpany_address = None
    if conpany_user in ignore_text:
        conpany_user = None
    if conpany_phone in ignore_text:
        conpany_phone = None
    if conpany_active_date in ignore_text:
        conpany_active_date = None
    if conpany_manage in ignore_text:
        conpany_manage = None
    if conpany_category in ignore_text:
        conpany_category = None
    if conpany_status in ignore_text:
        conpany_status = None
    if conpany_last_update in ignore_text:
        conpany_last_update = None

    # print('Ng√†nh ngh·ªÅ kinh doanh')
    company_career_code = []
    company_career_name = []
    for company_career in company_career_els:
        career_code_info = company_career.xpath(".//td[1]//a")[0]
        career_code = career_code_info.text_content().strip()
        company_career_code.append(career_code)
        company_career_name.append(company_career.xpath(".//td[2]//a")[0].text_content().strip())
    company_career_codes = ', '.join(company_career_code)
    company_career_names = ', '.join(company_career_name)

    company_data = {
        'name': company_name,
        'name_short': company_name_short,
        'name_global': company_name_globe,
        'tax': conpany_tax,
        'address': conpany_address,
        # 'district_id': district_id,
        # 'province_id': province_id,
        'representative': conpany_user,
        'phone': conpany_phone,
        'active_date': conpany_active_date,
        'manage_by': conpany_manage,
        'category': conpany_category,
        'status': conpany_status,
        'last_update': conpany_last_update,
        'career_code': company_career_codes,
        'career_name': company_career_names,
        'slug': url
    }
    # In th√¥ng tin v·ªõi format ƒë·∫πp
    print_company_info(company_data)
    
    return company_data

    # print(company_career_codes)
    # print(', '.join(company_career_names))

    # sql = """
    #     INSERT INTO mst_company (name, name_short, name_global, tax, address, district_id, province_id, representative, phone, active_date, manage_by, category, status, last_update, career_code, career_name, slug)
    #     VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
    # """
    # cur.execute(sql, (
    #     company_name, company_name_short, company_name_globe, conpany_tax,
    #     conpany_address, district_id, province_id,
    #     conpany_user, conpany_phone,
    #     conpany_active_date, conpany_manage, conpany_category, conpany_status, conpany_last_update,
    #     company_career_codes, company_career_names, url))
    # conn.commit()

def print_company_info(data):
    """In th√¥ng tin c√¥ng ty v·ªõi format ƒë·∫πp"""
    print("\n" + "="*80)
    print("üìä TH√îNG TIN C√îNG TY")
    print("="*80)
    
    # Th√¥ng tin c∆° b·∫£n
    print(f"üè¢ T√™n c√¥ng ty        : {data.get('name', 'N/A')}")
    print(f"üìù T√™n vi·∫øt t·∫Øt       : {data.get('name_short', 'N/A')}")
    print(f"üåê T√™n qu·ªëc t·∫ø        : {data.get('name_global', 'N/A')}")
    print(f"üî¢ M√£ s·ªë thu·∫ø        : {data.get('tax', 'N/A')}")
    
    print("\n" + "-"*50)
    print("üìç ƒê·ªäA CH·ªà & LI√äN H·ªÜ")
    print("-"*50)
    print(f"üè† ƒê·ªãa ch·ªâ           : {data.get('address', 'N/A')}")
    # print(f"üèòÔ∏è  Qu·∫≠n/Huy·ªán        : {data.get('district_id', 'N/A')}")
    # print(f"üó∫Ô∏è  T·ªânh/Th√†nh ph·ªë    : {data.get('province_id', 'N/A')}")
    print(f"üë§ Ng∆∞·ªùi ƒë·∫°i di·ªán     : {data.get('representative', 'N/A')}")
    print(f"üìû S·ªë ƒëi·ªán tho·∫°i      : {data.get('phone', 'N/A')}")
    
    print("\n" + "-"*50)
    print("üìã TH√îNG TIN DOANH NGHI·ªÜP")
    print("-"*50)
    print(f"üìÖ Ng√†y ho·∫°t ƒë·ªông     : {data.get('active_date', 'N/A')}")
    print(f"üèõÔ∏è  C∆° quan qu·∫£n l√Ω    : {data.get('manage_by', 'N/A')}")
    print(f"üìÇ Lo·∫°i h√¨nh         : {data.get('category', 'N/A')}")
    print(f"‚úÖ Tr·∫°ng th√°i        : {data.get('status', 'N/A')}")
    print(f"üîÑ C·∫≠p nh·∫≠t cu·ªëi     : {data.get('last_update', 'N/A')}")
    
    print("\n" + "-"*50)
    print("üíº NG√ÄNH NGH·ªÄ")
    print("-"*50)
    print(f"üîñ M√£ ng√†nh ngh·ªÅ     : {data.get('career_code', 'N/A')}")
    print(f"üìä T√™n ng√†nh ngh·ªÅ    : {data.get('career_name', 'N/A')}")
    
    print("\n" + "-"*50)
    print("üîó KH√ÅC")
    print("-"*50)
    print(f"üåê Slug              : {data.get('slug', 'N/A')}")
    
    print("="*80)
    print("‚úÖ HO√ÄN TH√ÄNH CRAWL TH√îNG TIN C√îNG TY")
    print("="*80 + "\n")

def generate_slug(ten_cty, mst):
    clean_mst = str(mst).strip()
    clean_ten_cty = str(ten_cty).strip()
    slug = unidecode(clean_ten_cty)
    slug = slug.replace("&", "-").replace(".", "-").replace("'", "-")
    slug = re.sub(r"[^a-zA-Z0-9\s-]", "", slug)
    slug = re.sub(r"\s+", "-", slug)
    slug = re.sub(r"-+", "-", slug)
    slug = slug.strip("-")
    slug = slug.lower()

    return f"/{clean_mst}-{slug}"

def read_excel_and_generate_slugs(file_path):
    try:
        # ƒê·ªçc file Excel
        df = pd.read_excel(file_path)
        
        # L·∫•y c·ªôt D (t√™n c√¥ng ty) v√† c·ªôt E (m√£ s·ªë thu·∫ø)
        # Pandas s·ª≠ d·ª•ng index t·ª´ 0, n√™n c·ªôt D = index 3, c·ªôt E = index 4
        ten_cty_col = df.iloc[:, 3]  # C·ªôt D
        mst_col = df.iloc[:, 4]      # C·ªôt E
        
        # T·∫°o danh s√°ch c√°c slug
        slugs = []
        for i in range(len(df)):
            ten_cty = ten_cty_col.iloc[i]
            mst = mst_col.iloc[i]
            
            # B·ªè qua c√°c d√≤ng c√≥ gi√° tr·ªã null
            if pd.notna(ten_cty) and pd.notna(mst):
                slug = generate_slug(ten_cty, mst)
                slugs.append({
                    'ten_cong_ty': ten_cty,
                    'ma_so_thue': mst,
                    'slug': slug
                })
        
        return slugs
    
    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc file: {e}")
        return []
    
def save_to_csv_pandas(results, output_file="company_slugs.csv"):
    try:
        df = pd.DataFrame(results)
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"ƒê√£ l∆∞u {len(results)} d√≤ng d·ªØ li·ªáu v√†o file {output_file}")
        
    except Exception as e:
        print(f"L·ªói khi l∆∞u file CSV: {e}")

def save_company_data_to_csv_pandas(company_data_list, output_file="company_data.csv"):
    try:
        df = pd.DataFrame(company_data_list)
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"ƒê√£ l∆∞u {len(company_data_list)} c√¥ng ty v√†o file {output_file}")
        
    except Exception as e:
        print(f"L·ªói khi l∆∞u file CSV: {e}")


def split_csv_into_batches(input_file: str, batch_size: int = 100, output_dir: str = "batches") -> List[str]:
    """
    T√°ch file CSV th√†nh c√°c batch nh·ªè h∆°n
    
    Args:
        input_file: ƒê∆∞·ªùng d·∫´n file CSV g·ªëc
        batch_size: S·ªë l∆∞·ª£ng items trong m·ªói batch
        output_dir: Th∆∞ m·ª•c l∆∞u c√°c file batch
    
    Returns:
        List ƒë∆∞·ªùng d·∫´n c√°c file batch ƒë√£ t·∫°o
    """
    # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a t·ªìn t·∫°i
    os.makedirs(output_dir, exist_ok=True)
    
    # ƒê·ªçc file CSV g·ªëc
    df = pd.read_csv(input_file)
    total_rows = len(df)
    num_batches = math.ceil(total_rows / batch_size)
    
    batch_files = []
    
    for i in range(num_batches):
        start_idx = i * batch_size
        end_idx = min((i + 1) * batch_size, total_rows)
        
        # L·∫•y subset data cho batch hi·ªán t·∫°i
        batch_df = df.iloc[start_idx:end_idx]
        
        # T·∫°o t√™n file cho batch
        batch_filename = f"company_slugs_batch_{i+1:03d}.csv"
        batch_filepath = os.path.join(output_dir, batch_filename)
        
        # L∆∞u batch v√†o file
        batch_df.to_csv(batch_filepath, index=False)
        batch_files.append(batch_filepath)
        
        logging.info(f"Created batch {i+1}/{num_batches}: {batch_filename} ({len(batch_df)} items)")
    
    return batch_files

def process_company_slugs_in_batches(input_csv: str, batch_size: int = 100):
    """
    X·ª≠ l√Ω file company_slugs.csv theo t·ª´ng batch
    
    Args:
        input_csv: ƒê∆∞·ªùng d·∫´n file company_slugs.csv g·ªëc
        batch_size: S·ªë l∆∞·ª£ng items trong m·ªói batch
    """
    logging.info(f"Starting batch processing for {input_csv}")
    
    # T√°ch file th√†nh c√°c batch
    batch_files = split_csv_into_batches(input_csv, batch_size)
    
    # T·∫°o th∆∞ m·ª•c ƒë·ªÉ l∆∞u k·∫øt qu·∫£
    results_dir = "crawled_results"
    os.makedirs(results_dir, exist_ok=True)
    
    # X·ª≠ l√Ω t·ª´ng batch
    for batch_idx, batch_file in enumerate(batch_files, 1):
        try:
            logging.info(f"Processing batch {batch_idx}/{len(batch_files)}: {batch_file}")
            
            # ƒê·ªçc batch hi·ªán t·∫°i
            batch_df = pd.read_csv(batch_file)
            
            # Crawl data cho batch n√†y
            crawled_data = crawl_batch_data(batch_df)
            
            # T·∫°o t√™n file k·∫øt qu·∫£
            result_filename = f"crawled_batch_{batch_idx:03d}.csv"
            result_filepath = os.path.join(results_dir, result_filename)
            
            # L∆∞u k·∫øt qu·∫£ crawl
            crawled_data.to_csv(result_filepath, index=False)
            
            logging.info(f"Completed batch {batch_idx}: Saved to {result_filepath}")
            
        except Exception as e:
            logging.error(f"Error processing batch {batch_idx}: {str(e)}")
            continue
    
    logging.info("Batch processing completed")

def crawl_batch_data(batch_df: pd.DataFrame) -> pd.DataFrame:
    """
    Crawl d·ªØ li·ªáu cho m·ªôt batch companies
    
    Args:
        batch_df: DataFrame ch·ª©a company slugs c·ªßa batch
    
    Returns:
        DataFrame ch·ª©a d·ªØ li·ªáu ƒë√£ crawl
    """
    crawled_results = []
    
    for idx, row in batch_df.iterrows():
        try:
            company_slug = row['slug']  # Gi·∫£ s·ª≠ column name l√† 'slug'
            
            # G·ªçi h√†m crawl cho t·ª´ng company (thay th·∫ø b·∫±ng h√†m crawl th·ª±c t·∫ø)
            company_data = crawl_single_company(company_slug)
            
            if company_data:
                crawled_results.append(company_data)
                
            logging.info(f"Crawled data for: {company_slug}")
            
        except Exception as e:
            logging.error(f"Error crawling {company_slug}: {str(e)}")
            continue
    
    return pd.DataFrame(crawled_results)

def crawl_single_company(company_slug: str) -> dict:
    """
    Crawl d·ªØ li·ªáu cho m·ªôt company c·ª• th·ªÉ
    
    Args:
        company_slug: Slug c·ªßa company
    
    Returns:
        Dictionary ch·ª©a d·ªØ li·ªáu company
    """
    # V√≠ d·ª•:
    return crawl_data_company_by_url(company_slug, const_headers)
    # return crawl_company_details(company_slug)
    

def merge_batch_results(results_dir: str = "crawled_results", output_file: str = "final_crawled_data.csv"):
    """
    G·ªôp t·∫•t c·∫£ k·∫øt qu·∫£ t·ª´ c√°c batch th√†nh m·ªôt file cu·ªëi c√πng
    
    Args:
        results_dir: Th∆∞ m·ª•c ch·ª©a c√°c file k·∫øt qu·∫£ batch
        output_file: T√™n file k·∫øt qu·∫£ cu·ªëi c√πng
    """
    batch_files = [f for f in os.listdir(results_dir) if f.startswith('crawled_batch_')]
    batch_files.sort()
    
    all_data = []
    
    for batch_file in batch_files:
        batch_path = os.path.join(results_dir, batch_file)
        batch_df = pd.read_csv(batch_path)
        all_data.append(batch_df)
        logging.info(f"Loaded {len(batch_df)} records from {batch_file}")
    
    # G·ªôp t·∫•t c·∫£ data
    final_df = pd.concat(all_data, ignore_index=True)
    final_df.to_csv(output_file, index=False)
    
    logging.info(f"Merged {len(final_df)} total records into {output_file}")




# L·∫•y th√¥ng tin v·ªÅ t·ªânh/th√†nh ph·ªë
# crawl_data_province(pattern.URL_PATH_BY_PROVINCE, const_headers) # =============================================== Job
# L·∫•y th√¥ng tin qu·∫≠n huy·ªán theo URL
# crawl_data_district() # ========================================================================================== Job
# crawl_data_district_by_url('/tra-cuu-ma-so-thue-theo-tinh/ha-noi-7', const_headers) # Test


# L·∫•y th√¥ng tin ng√†nh ngh·ªÅ
# crawl_data_career(pattern.URL_PATH_BY_CAREER, const_headers) # =================================================== Job

# ƒê·ªçc file Excel v√† t·∫°o slug
# file_path = "/Users/taidang/Desktop/hanoi_FULL.xlsx"  # Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n file
# results = read_excel_and_generate_slugs(file_path)

# In k·∫øt qu·∫£
# for item in results:
#     print(f"C√¥ng ty: {item['ten_cong_ty']}")
#     print(f"MST: {item['ma_so_thue']}")
#     print(f"Slug: {item['slug']}")
#     print("-" * 50)

# L∆∞u company slug v√†o file CSV
# save_to_csv_pandas(results, "company_slugs.csv")

# L·∫•y th√¥ng tin m√£ s·ªë thu·∫ø c√¥ng ty t·ª´ file CSV v√† crawl d·ªØ li·ªáu
# process_company_slugs("company_slugs.csv", "company_data.csv")



# L·∫•y th√¥ng tin c√¥ng ty
# crawl_data_company() # =========================================================================================== Job
# crawl_data_company_by_url('/2100689933-cong-ty-tnhh-mtv-vang-bac-kim-hue', const_headers)
# crawl_data_company_by_url('/2100689059-cong-ty-tnhh-xang-dau-tra-vinh-petro', const_headers)
# crawl_data_company_by_url('/0315739605-001-van-phong-dai-dien-cong-ty-tnhh-chint-vietnam-holding-tai-ha-noi', const_headers)
process_company_slugs_in_batches("company_slugs.csv", batch_size=100)
# def main():
#     """H√†m main ƒë·ªÉ ch·∫°y batch processing"""
    
#     # Thay v√¨ x·ª≠ l√Ω to√†n b·ªô file m·ªôt l√∫c
#     # process_company_slugs("company_slugs.csv")
    
#     # S·ª≠ d·ª•ng batch processing
#     # from batch_processor import process_company_slugs_in_batches
    
#     process_company_slugs_in_batches("company_slugs.csv", batch_size=100)
